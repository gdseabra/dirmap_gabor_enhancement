{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Imports and System Path Setup\n",
    "This cell now includes the yaml library for parsing your config files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import yaml  # <-- Added for YAML loading\n",
    "\n",
    "# --- IMPORTANT ---\n",
    "# Add the root directory of your project to the system path\n",
    "# This should be the '/home/seabra/dirmap_gabor_enhancement/' directory\n",
    "PROJECT_PATH = '/home/seabra/dirmap_gabor_enhancement/'\n",
    "sys.path.append(PROJECT_PATH)\n",
    "sys.path.append(os.path.join(PROJECT_PATH, 'src'))\n",
    "\n",
    "# Import your custom modules\n",
    "try:\n",
    "    from data.enhancer_train_datamodule import EnhancerTrainDataModule\n",
    "    from models.components.DirmapNet import DirmapNet\n",
    "    from models.dirmap_module import DirmapLitModule\n",
    "    from models.components.UNet import UNet\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing modules: {e}\")\n",
    "    print(f\"Please ensure PROJECT_PATH is set correctly. Current path: {PROJECT_PATH}\")\n",
    "\n",
    "# Helper function for visualization (unchanged)\n",
    "def visualize_batch(batch, predictions=None, title=\"Data Batch Visualization\"):\n",
    "    \"\"\"Helper function to visualize a sample from a batch.\"\"\"\n",
    "    # Unpack the batch\n",
    "    lat, dirmap_target_idx, mask = batch\n",
    "    \n",
    "    # Select the first item in the batch for visualization\n",
    "    lat = lat[0].cpu().squeeze()\n",
    "    dirmap_target_idx = dirmap_target_idx[0].cpu().squeeze()\n",
    "    mask = mask[0].cpu().squeeze()\n",
    "    \n",
    "    num_plots = 3\n",
    "    if predictions:\n",
    "        num_plots += 2 # Add plots for predictions\n",
    "\n",
    "    fig, axs = plt.subplots(1, num_plots, figsize=(20, 4))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    axs[0].imshow(lat, cmap='gray')\n",
    "    axs[0].set_title('Input Latent')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(dirmap_target_idx, cmap='jet', vmin=0, vmax=dirmap_target_idx.max())\n",
    "    axs[1].set_title('GT Dirmap (Index)')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    axs[2].imshow(mask, cmap='gray')\n",
    "    axs[2].set_title('Mask')\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    if predictions:\n",
    "        pred_dirmap, gabor_map = predictions\n",
    "        \n",
    "        pred_dirmap_idx = torch.argmax(torch.sigmoid(pred_dirmap[0]), dim=0).cpu().detach()\n",
    "        axs[3].imshow(pred_dirmap_idx, cmap='jet', vmin=0, vmax=89)\n",
    "        axs[3].set_title('Pred Dirmap (Index)')\n",
    "        axs[3].axis('off')\n",
    "\n",
    "        pred_gabor = gabor_map[0, 0, :, :].cpu().detach()\n",
    "        axs[4].imshow(pred_gabor, cmap='gray')\n",
    "        axs[4].set_title('Pred Gabor')\n",
    "        axs[4].axis('off')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Imports and setup complete. ‚úÖ\")\n",
    "print(f\"Project Path: {PROJECT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Configuration Loading from YAML\n",
    "This is the new cell that loads your configurations from the specified YAML files. It also sets the DEVICE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration File Paths ---\n",
    "DATA_CONFIG_PATH = os.path.join(PROJECT_PATH, \"configs/data/enhancer_train.yaml\")\n",
    "MODEL_CONFIG_PATH = os.path.join(PROJECT_PATH, \"configs/model/dirmap_estimator.yaml\")\n",
    "\n",
    "# --- Helper function to load YAML ---\n",
    "def load_config(config_path):\n",
    "    \"\"\"Loads a YAML config file.\"\"\"\n",
    "    if not os.path.exists(config_path):\n",
    "        print(f\"‚ùå Config file not found at: {config_path}\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        return config\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading YAML from {config_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Load Configurations ---\n",
    "print(\"Loading configurations...\")\n",
    "data_config = load_config(DATA_CONFIG_PATH)\n",
    "model_config = load_config(MODEL_CONFIG_PATH)\n",
    "\n",
    "# --- Set Device ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if data_config and model_config:\n",
    "    print(\"Successfully loaded data and model configs. üéâ\\n\")\n",
    "    print(\"--- Data Config Keys ---\")\n",
    "    print(data_config.keys())\n",
    "    print(\"\\n--- Model Config Keys ---\")\n",
    "    print(model_config.keys())\n",
    "    print(f\"\\nUsing device: {DEVICE}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Failed to load one or more config files. Please check paths and file integrity.\")\n",
    "\n",
    "# --- Manual Overrides for Debugging ---\n",
    "# We use a small batch size for easier debugging\n",
    "if data_config:\n",
    "    data_config['batch_size'] = 1\n",
    "    data_config['num_workers'] = 4\n",
    "    print(f\"\\nApplied debug overrides: batch_size={data_config['batch_size']}, num_workers={data_config['num_workers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Load and Inspect the Data (from Config)\n",
    "This cell now instantiates EnhancerTrainDataModule by unpacking the data_config dictionary. It also intelligently handles the data_dir path, assuming it's relative to your PROJECT_PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instantiate the DataModule using the loaded config\n",
    "try:\n",
    "    # --- FIX 1: Remove the '_target_' key if it exists ---\n",
    "    data_config.pop('_target_', None)\n",
    "\n",
    "    # --- FIX 2: Resolve the data_dir path variable ---\n",
    "    # The simple yaml loader doesn't resolve ${...} variables. We'll replace it manually.\n",
    "    # We assume the base data directory is named 'data' inside your project path.\n",
    "    # if 'data_dir' in data_config and '${paths.data_dir}' in data_config['data_dir']:\n",
    "    #     print(f\"Resolving data_dir variable from: {data_config['data_dir']}\")\n",
    "    #     # Replace the variable part with a default 'data' folder\n",
    "    #     resolved_path = data_config['data_dir'].replace('${paths.data_dir}', \"/storage/seabra/segmentation_study/train_data/\")\n",
    "    #     data_config['data_dir'] = os.path.join(PROJECT_PATH, resolved_path)\n",
    "    # else:\n",
    "    #     # Fallback for paths that don't use the variable\n",
    "    #     data_config['data_dir'] = os.path.join(PROJECT_PATH, data_config.get('data_dir', \"/storage/seabra/segmentation_study/train_data/\"))\n",
    "\n",
    "    data_config['data_dir'] = \"/storage/seabra/segmentation_study/train_data/\"\n",
    "    print(f\"Absolute data_dir set to: {data_config['data_dir']}\")\n",
    "\n",
    "    # Now, instantiate the data module\n",
    "    data_module = EnhancerTrainDataModule(**data_config)\n",
    "    \n",
    "    # 2. Setup the datasets\n",
    "    data_module.setup()\n",
    "\n",
    "    # 3. Get the training dataloader\n",
    "    train_dataloader = data_module.train_dataloader()\n",
    "\n",
    "    # 4. Fetch one batch of data\n",
    "    batch = next(iter(train_dataloader))\n",
    "    print(\"\\nSuccessfully loaded one batch of data! üéâ\\n\")\n",
    "    \n",
    "    # 5. Inspect the batch contents\n",
    "    x, target_dirmap_idx, mask = batch\n",
    "    print(f\"Input latent 'x' shape: \\t\\t{x.shape}, type: {x.dtype}\")\n",
    "    print(f\"Target dirmap 'target_dirmap_idx' shape: \\t{target_dirmap_idx.shape}, type: {target_dirmap_idx.dtype}\")\n",
    "    print(f\"Mask 'mask' shape: \\t\\t\\t{mask.shape}, type: {mask.dtype}\")\n",
    "    \n",
    "    # 6. Visualize the batch\n",
    "    visualize_batch(batch, title=\"Ground Truth Data from a Single Batch\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error setting up data: {e}\")\n",
    "    print(\"Please check your 'enhancer_train.yaml' config and file paths.\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Unpacks the batch tuple into its components.\n",
    "# x: Input tensor (e.g., images), shape (B, C_in, H_small, W_small)\n",
    "# true_dirmap_labels: Ground truth orientation labels at a small resolution\n",
    "#                     Shape: (B, 1, H/8, W/8)\n",
    "# y_orig, y_bin: Ground truth for enhancement (original and binary)\n",
    "# roi_mask: Region of Interest mask, full resolution, Shape: (B, 1, H, W)\n",
    "x, true_dirmap_labels, roi_mask = batch\n",
    "\n",
    "# Downsamples the full-resolution ROI mask to match the initial label map size (1/8th scale)\n",
    "# Output Shape: (B, 1, H/8, W/8)\n",
    "small_dirmap_mask = F.interpolate(roi_mask, scale_factor=1/8, mode=\"bilinear\") \n",
    "\n",
    "# --- Start of Complex Target Preparation ---\n",
    "\n",
    "# 'indices' holds the low-res angle labels. Shape: (B, 1, H/8, W/8)\n",
    "indices = true_dirmap_labels \n",
    "\n",
    "# Finds all pixels with odd-numbered angles\n",
    "is_odd = indices % 2 != 0\n",
    "# Rounds odd angles down (e.g., 179 -> 178)\n",
    "indices[is_odd] -= 1\n",
    "# Maps angle ranges [0,1], [2,3]...[178,179] to classes 0, 1... 89\n",
    "indices = indices // 2\n",
    "# 'indices' now holds classes 0-89. Shape: (B, 1, H/8, W/8)\n",
    "\n",
    "# Sets all pixels *outside* the downsampled ROI to class 90\n",
    "# Class 90 will serve as the \"ignore\" or \"background\" class\n",
    "indices[small_dirmap_mask == 0] = 90\n",
    "\n",
    "\n",
    "# Creates a binary mask. Pixels are 1 (keep) if they are NOT the\n",
    "# 'ignore' class (90), and 0 (ignore) if they are.\n",
    "# Output Shape: (B, 1, H/8, W/8). \n",
    "dirmap_mask = (indices != 90).long()\n",
    "\n",
    "# Sets the 'ignore' class pixels (90) to 0.\n",
    "# This is safe because 'dirmap_mask' already knows to ignore them.\n",
    "# This step makes the labels (0-89) compatible with the\n",
    "# 90-class prediction 'pred_dirmap'.\n",
    "indices[indices == 90] = 0\n",
    "\n",
    "pre_processed_labels_batch = (x, indices, dirmap_mask)\n",
    "\n",
    "visualize_batch(pre_processed_labels_batch, title=\"Ground Truth Data from a Single Batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices[small_dirmap_mask == 0] = 0\n",
    "plt.imshow(indices[0][0].cpu().detach()*2, cmap='jet')\n",
    "plt.colorbar(label='Angle (degrees)', ticks=[_ for _ in range(0, 180, 10)])\n",
    "plt.title('Orientation Field Label (blockwise)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Initialize Model and LightningModule (from Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ‚ùóÔ∏è CONFIGURATION: SET YOUR WEIGHTS FILE PATH HERE ---\n",
    "# Set the full path to your pretrained model .pth or .ckpt file.\n",
    "# If you don't want to load weights, set this to None.\n",
    "PRETRAINED_WEIGHTS_PATH = \"/home/seabra/dirmap_gabor_enhancement/logs/train/runs/2025-11-11_21-27-42/checkpoints/last.ckpt\"\n",
    "# Example: PRETRAINED_WEIGHTS_PATH = \"/home/seabra/dirmap_gabor_enhancement/logs/train/runs/2025-11-11_21-27-42/checkpoints/epoch_005.ckpt\"\n",
    "\n",
    "try:\n",
    "    # --- Clean up top-level Hydra key ---\n",
    "    model_config.pop('_target_', None)\n",
    "\n",
    "    # --- 1. Instantiate the network from the 'net' config block ---\n",
    "    print(\"Initializing model architecture...\")\n",
    "    if 'net' in model_config and isinstance(model_config['net'], dict):\n",
    "        net_config = model_config['net'].copy()\n",
    "        net_config.pop('_target_', None) \n",
    "        net_params = net_config\n",
    "    else:\n",
    "        net_params = {}\n",
    "\n",
    "    net = DirmapNet(**net_params).to(DEVICE)\n",
    "    print(f\"  -> Net: DirmapNet with params: {net_params}\")\n",
    "\n",
    "    # --- 2. LOAD PRETRAINED WEIGHTS (NEW SECTION) ---\n",
    "    if PRETRAINED_WEIGHTS_PATH and os.path.exists(PRETRAINED_WEIGHTS_PATH):\n",
    "        print(f\"\\nAttempting to load weights from: {PRETRAINED_WEIGHTS_PATH}\")\n",
    "        try:\n",
    "            # Load the checkpoint, mapping it to the correct device\n",
    "            checkpoint = torch.load(PRETRAINED_WEIGHTS_PATH, map_location=DEVICE)\n",
    "            \n",
    "            # Check if the checkpoint is a dict and has 'state_dict' (standard for PyTorch Lightning)\n",
    "            if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n",
    "                pretrained_state_dict = checkpoint['state_dict']\n",
    "            else:\n",
    "                # Assume the file is just the state_dict itself\n",
    "                pretrained_state_dict = checkpoint\n",
    "\n",
    "            # --- Handle key mismatches (very important!) ---\n",
    "            # Often, weights saved from a LightningModule have a prefix like \"net.\"\n",
    "            # We need to remove this prefix to match the keys in our standalone `net` object.\n",
    "            new_state_dict = {}\n",
    "            for k, v in pretrained_state_dict.items():\n",
    "                if k.startswith(\"net.\"):\n",
    "                    new_state_dict[k.replace(\"net.\", \"\", 1)] = v\n",
    "                else:\n",
    "                    # Keep keys that don't have the prefix (in case it was saved differently)\n",
    "                    new_state_dict[k] = v\n",
    "            \n",
    "            # Load the state_dict. `strict=False` is safer as it ignores non-matching keys\n",
    "            # (e.g., if you are not loading the enhancer part, or if the optimizer state is missing).\n",
    "            missing_keys, unexpected_keys = net.load_state_dict(new_state_dict, strict=False)\n",
    "            print(\"  -> Pretrained weights loaded successfully! ‚úÖ\")\n",
    "            if missing_keys:\n",
    "                print(f\"    - Note: Some keys were missing in the checkpoint: {missing_keys[:3]}...\")\n",
    "            if unexpected_keys:\n",
    "                print(f\"    - Note: Some keys in the checkpoint were not in the model: {unexpected_keys[:3]}...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  -> ‚ùå Error loading weights: {e}\")\n",
    "    elif PRETRAINED_WEIGHTS_PATH:\n",
    "        print(f\"\\n‚ö†Ô∏è Warning: Pretrained weights file not found at '{PRETRAINED_WEIGHTS_PATH}'. Skipping.\")\n",
    "\n",
    "\n",
    "    # --- 3. Define Optimizer Callables (Unchanged) ---\n",
    "    print(\"\\nParsing optimizer configs...\")\n",
    "    opt_cfg = model_config['optimizer'].copy()\n",
    "    opt_cfg.pop('_target_', None); opt_cfg.pop('_partial_', None)\n",
    "    optimizer_callable = partial(torch.optim.AdamW, **opt_cfg)\n",
    "    print(f\"  -> Optimizer: AdamW with params: {opt_cfg}\")\n",
    "\n",
    "    # --- 4. Define Scheduler Callables (Unchanged) ---\n",
    "    print(\"Parsing scheduler configs...\")\n",
    "    sched_cfg = model_config['scheduler'].copy()\n",
    "    sched_cfg.pop('_target_', None); sched_cfg.pop('_partial_', None)\n",
    "    scheduler_callable = partial(torch.optim.lr_scheduler.ReduceLROnPlateau, **sched_cfg)\n",
    "    print(f\"  -> Scheduler: ReduceLROnPlateau with params: {sched_cfg}\")\n",
    "\n",
    "    # --- 5. Get other HPs for the LitModule (Unchanged) ---\n",
    "    keys_to_exclude = [\n",
    "        'optimizer', 'scheduler', 'net', 'compile'\n",
    "    ]\n",
    "    lit_module_hps = {k: v for k, v in model_config.items() if k not in keys_to_exclude}\n",
    "    print(f\"\\nPassing other HPs to LitModule: {list(lit_module_hps.keys())}\")\n",
    "\n",
    "    # --- 6. Instantiate the LightningModule (Unchanged) ---\n",
    "    lit_model = DirmapLitModule(\n",
    "        net=net, # Pass the network with the newly loaded weights\n",
    "        optimizer=optimizer_callable,\n",
    "        scheduler=scheduler_callable,\n",
    "        compile=False,\n",
    "        **lit_module_hps\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    print(\"\\nModel and LightningModule initialized successfully. üöÄ\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error initializing model from config: {e}\")\n",
    "    print(\"Please check your 'enhancer.yaml' config.\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Manual Forward Pass (model_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure the 'batch' variable exists from running the cell above\n",
    "if 'batch' not in locals():\n",
    "    print(\"‚ùå 'batch' is not defined. Please re-run Cell 3 to load the data first.\")\n",
    "else:\n",
    "    # Move the batch to the same device as the model\n",
    "    x, target_dirmap_idx, mask = [t.to(DEVICE) for t in batch]\n",
    "    device_batch = (x, target_dirmap_idx, mask)\n",
    "\n",
    "    # --- Perform a single forward pass and calculate losses ---\n",
    "    print(\"Running a single `model_step` (Forward Pass + Loss Calculation)...\")\n",
    "\n",
    "    print(target_dirmap_idx.shape, mask.shape)\n",
    "\n",
    "    try:\n",
    "        # Get the dictionary of losses\n",
    "        losses = lit_model.model_step(device_batch)\n",
    "        \n",
    "        # Inspect the calculated losses\n",
    "        total_loss = losses\n",
    "\n",
    "        print(\"\\n--- Calculated Losses ---\")\n",
    "        print(f\"Total Loss: \\t\\t{total_loss:.4f}\")\n",
    "        \n",
    "        # Also, let's get the model's output for visualization\n",
    "        with torch.no_grad():\n",
    "            pred_dirmap, pred_gabor, = lit_model.forward(x)\n",
    "            predictions = (pred_dirmap, pred_gabor)\n",
    "            visualize_batch(device_batch, predictions, \"Model Predictions vs. Ground Truth\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during model_step: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# def gaussian_kernel1d(sigma: float, truncate: float = 3.0, device=None):\n",
    "#     \"\"\"Creates a 1D Gaussian kernel normalized to sum to 1.\"\"\"\n",
    "#     radius = int(truncate * sigma + 0.5)\n",
    "#     x = torch.arange(-radius, radius + 1, device=device, dtype=torch.float32)\n",
    "#     kernel = torch.exp(-0.5 * (x / sigma) ** 2)\n",
    "#     kernel /= kernel.sum()\n",
    "#     return kernel\n",
    "\n",
    "\n",
    "# def gaussian_blur2d(x: torch.Tensor, sigma: float):\n",
    "#     \"\"\"Applies Gaussian blur to a 4D tensor (B, C, H, W) using separable convs.\"\"\"\n",
    "#     B, C, H, W = x.shape\n",
    "#     device = x.device\n",
    "\n",
    "#     # Create 1D kernels\n",
    "#     kernel_1d = gaussian_kernel1d(sigma, device=device)\n",
    "#     k = kernel_1d.view(1, 1, -1, 1)  # vertical\n",
    "#     k_t = kernel_1d.view(1, 1, 1, -1)  # horizontal\n",
    "\n",
    "#     # Apply separable convs (per channel)\n",
    "#     x = F.conv2d(x, k.expand(C, 1, -1, 1), groups=C, padding=(kernel_1d.numel() // 2, 0))\n",
    "#     x = F.conv2d(x, k_t.expand(C, 1, 1, -1), groups=C, padding=(0, kernel_1d.numel() // 2))\n",
    "#     return x\n",
    "\n",
    "# def gaussian_soft_label(target, sigma=2.0, scale_factor=8, num_classes=8):\n",
    "#     \"\"\"\n",
    "#     Converts block-level labels (B, 1, H//8, W//8) into a smooth Gaussian probability map (B, C, H, W).\n",
    "#     \"\"\"\n",
    "#     B, _, h, w = target.shape\n",
    "#     device = target.device\n",
    "\n",
    "#     # One-hot encode coarse labels\n",
    "#     target_onehot = F.one_hot(target.squeeze(1).long(), num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
    "\n",
    "#     # Upsample with nearest to the final size first\n",
    "#     coarse = F.interpolate(target_onehot, scale_factor=scale_factor, mode='nearest')\n",
    "\n",
    "#     # Apply Gaussian smoothing per class channel\n",
    "#     kernel_size = int(6*sigma + 1)\n",
    "#     padding = kernel_size // 2\n",
    "#     smoothed = gaussian_blur2d(coarse, sigma=sigma)\n",
    "\n",
    "#     # Normalize so each pixel‚Äôs probabilities sum to 1\n",
    "#     smoothed = smoothed / (smoothed.sum(dim=1, keepdim=True) + 1e-8)\n",
    "#     return smoothed\n",
    "\n",
    "# # One-hot encode and upsample with bilinear interpolation\n",
    "# gt_onehot = F.one_hot(target_dirmap_idx.squeeze(1).long(), num_classes=90).permute(0, 3, 1, 2).float()\n",
    "# gt_soft = F.interpolate(gt_onehot, scale_factor=8, mode='bilinear', align_corners=False)\n",
    "\n",
    "# gt_hard = F.interpolate(gt_onehot, scale_factor=8, mode='nearest')\n",
    "\n",
    "# # Normalize soft labels\n",
    "# gt_soft = gt_soft / (gt_soft.sum(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "\n",
    "# plt.imshow(gt_hard[0, 44, :, :].cpu().detach(), cmap='gray')             # imagem de base\n",
    "# plt.imshow(gt_soft[0, 44, :, :].cpu().detach(), cmap='jet', alpha=0.5)   # sobreposi√ß√£o sem apagar o fundo\n",
    "# plt.axis('off')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_map = torch.sigmoid(pred_dirmap[0])\n",
    "max_probs, _ = torch.max(prob_map, dim=0)\n",
    "\n",
    "plt.imshow(max_probs.cpu().detach(), cmap='jet')\n",
    "plt.colorbar(label='Confidence (probability)')\n",
    "plt.title('Confidence Map (softmax normalized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pred_gabor.squeeze(1)[0].cpu().detach(), cmap='gray')\n",
    "plt.title('Intermediate Gabor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example tensor\n",
    "x = prob_map\n",
    "# Arbitrary pixel coordinates (list of tuples)\n",
    "pixels = [(35, 25), (32, 32), (40, 16)]  # N=3 pixels\n",
    "\n",
    "# Plot each pixel‚Äôs channel values\n",
    "fig, axes = plt.subplots(1, len(pixels), figsize=(15, 4))\n",
    "\n",
    "for ax, (i, j) in zip(axes, pixels):\n",
    "    channel_values = x[:, i, j].cpu().detach().numpy()  # shape (C,)\n",
    "    ax.bar([ang*2 for ang in range(len(channel_values))], channel_values, color='gray')\n",
    "    ax.set_title(f'Pixel ({i},{j})')\n",
    "    ax.set_xlabel('Angle')\n",
    "    ax.set_ylabel('Probability')\n",
    "    # ax.set_xticks([0, len(channel_values), len(channel_values)*2 - 2])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
